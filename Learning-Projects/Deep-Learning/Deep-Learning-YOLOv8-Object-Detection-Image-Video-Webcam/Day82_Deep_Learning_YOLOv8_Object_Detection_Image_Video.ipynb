{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83077255-f22c-4dbd-93af-5208fd3728fa",
   "metadata": {},
   "source": [
    "# YOLOv8 Object Detection – Complete Beginner Walkthrough \n",
    "\n",
    "This notebook explains and implements **YOLOv8 (You Only Look Once)** for object detection.  \n",
    "\n",
    "We will cover:  \n",
    "\n",
    "- What is YOLO? Why use it?  \n",
    "- Difference between YOLO and traditional OpenCV Haar Cascades  \n",
    "- Pretrained models & datasets (COCO)  \n",
    "- Installing and setting up Ultralytics  \n",
    "- Running detection on images and videos  \n",
    "- Drawing bounding boxes with OpenCV  \n",
    "\n",
    "\n",
    "#  What is YOLO (You Only Look Once)?\n",
    "\n",
    "- **YOLO** is a state-of-the-art, real-time object detection algorithm.  \n",
    "- It divides an image into grids and predicts bounding boxes & class probabilities in a **single forward pass**.  \n",
    "- That’s why it’s **fast and accurate** → suitable for real-time detection.  \n",
    "\n",
    "> Use cases: self-driving cars, surveillance, medical imaging, quality inspection in factories, etc.\n",
    "\n",
    "\n",
    "#  YOLO vs Haar Cascade (OpenCV)\n",
    "\n",
    "- **Haar Cascades**:  \n",
    "\n",
    "  - Old method in OpenCV for object detection (e.g., face detection).  \n",
    "  - Works by scanning the image with sliding windows & handcrafted features.  \n",
    "  -  Limitations → only works well for simple tasks like faces; not robust for complex real-world detection.\n",
    "\n",
    "- **YOLO (Deep Learning-based)**: \n",
    "                                  \n",
    "  - Learns automatically from huge datasets (like COCO).  \n",
    "  -  Handles multiple objects, complex scenes, variations in lighting, orientation, etc.  \n",
    "  - Much more accurate & flexible than Haar Cascades.  \n",
    "\n",
    "> Conclusion: For modern applications → **YOLO is far better**.\n",
    "\n",
    "\n",
    "\n",
    "#  Pretrained Models and Datasets\n",
    "\n",
    "- Ultralytics provides **pretrained YOLO models** on the **COCO dataset** (80 object classes: person, car, dog, etc.).  \n",
    "- Different YOLOv8 model sizes: \n",
    "\n",
    "  - `yolov8n.pt` → Nano (fastest, lightest)  \n",
    "  - `yolov8s.pt` → Small  \n",
    "  - `yolov8m.pt` → Medium  \n",
    "  - `yolov8l.pt` → Large  \n",
    "  - `yolov8x.pt` → Extra large (most accurate but heavy)  \n",
    "\n",
    "You can start with pretrained weights and fine-tune them on your custom dataset later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb116c6-4f1e-4323-ac38-b25785661ef2",
   "metadata": {},
   "source": [
    "#  Setup Environment\n",
    "**This checks:**\n",
    "\n",
    "- Python version  \n",
    "- Torch (PyTorch) version  \n",
    "- GPU/CPU availability  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d687f8-2515-4de1-bf46-adafa2f9daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install YOLO library\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f4796a-5cb0-47e8-9197-359d0173116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.196  Python-3.13.5 torch-2.8.0+cpu CPU (Intel Core i3-5005U 2.00GHz)\n",
      "Setup complete  (4 CPUs, 7.9 GB RAM, 139.2/237.8 GB disk)\n"
     ]
    }
   ],
   "source": [
    "# Import and check environment\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fcf9d2-86ca-444e-8b9e-a6f88d83be15",
   "metadata": {},
   "source": [
    "# Load a Pretrained YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a178eebb-a8ac-4aab-bd79-3a67622ee786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# Load a pretrained YOLOv8 Nano model\n",
    "model = YOLO(\"yolov8n.pt\", \"v8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab203586-d2bb-4f89-b40d-0bc14a517cc0",
   "metadata": {},
   "source": [
    "# Run Prediction on an Image\n",
    "\n",
    "- `conf=0.25` → Minimum confidence threshold  \n",
    "- `save=True` → Saves results with bounding boxes to `runs/detect` folder  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44285c97-5e97-47f8-a1d5-782f473dac70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Lenovo\\OneDrive\\Desktop\\Python Everyday work\\Github work\\Computer_Vision\\YOLO\\Test_Image_and_Video\\Image.JPG: 416x640 30 persons, 10 cars, 1 bus, 3 trucks, 4 traffic lights, 1061.3ms\n",
      "Speed: 55.6ms preprocess, 1061.3ms inference, 26.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\Lenovo\\OneDrive\\Desktop\\Python Everyday work\\Github work\\Computer_Vision\\YOLO\\runs\\detect\\predict\u001b[0m\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[ 27,  28,  26],\n",
      "        [ 25,  26,  24],\n",
      "        [ 26,  27,  25],\n",
      "        ...,\n",
      "        [ 63,  72,  69],\n",
      "        [ 29,  37,  37],\n",
      "        [ 18,  28,  28]],\n",
      "\n",
      "       [[ 28,  29,  27],\n",
      "        [ 25,  26,  24],\n",
      "        [ 27,  28,  26],\n",
      "        ...,\n",
      "        [ 43,  50,  47],\n",
      "        [ 11,  19,  18],\n",
      "        [ 44,  55,  53]],\n",
      "\n",
      "       [[ 29,  30,  28],\n",
      "        [ 27,  28,  26],\n",
      "        [ 28,  29,  27],\n",
      "        ...,\n",
      "        [ 31,  38,  33],\n",
      "        [ 19,  29,  23],\n",
      "        [ 83,  92,  89]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 94,  83,  69],\n",
      "        [ 94,  83,  69],\n",
      "        [ 92,  81,  67],\n",
      "        ...,\n",
      "        [113, 107, 100],\n",
      "        [107, 101,  94],\n",
      "        [103,  97,  90]],\n",
      "\n",
      "       [[ 89,  78,  64],\n",
      "        [ 88,  77,  63],\n",
      "        [ 85,  74,  60],\n",
      "        ...,\n",
      "        [129, 123, 116],\n",
      "        [129, 123, 116],\n",
      "        [133, 127, 120]],\n",
      "\n",
      "       [[ 83,  72,  58],\n",
      "        [ 81,  70,  56],\n",
      "        [ 77,  66,  52],\n",
      "        ...,\n",
      "        [132, 126, 119],\n",
      "        [129, 123, 116],\n",
      "        [138, 132, 125]]], dtype=uint8)\n",
      "orig_shape: (582, 910)\n",
      "path: 'C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Python Everyday work\\\\Github work\\\\Computer_Vision\\\\YOLO\\\\Test_Image_and_Video\\\\Image.JPG'\n",
      "probs: None\n",
      "save_dir: 'C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Python Everyday work\\\\Github work\\\\Computer_Vision\\\\YOLO\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 55.56060001254082, 'inference': 1061.301099951379, 'postprocess': 26.681900024414062}]\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[ 27,  28,  26],\n",
      "        [ 25,  26,  24],\n",
      "        [ 26,  27,  25],\n",
      "        ...,\n",
      "        [ 63,  72,  69],\n",
      "        [ 29,  37,  37],\n",
      "        [ 18,  28,  28]],\n",
      "\n",
      "       [[ 28,  29,  27],\n",
      "        [ 25,  26,  24],\n",
      "        [ 27,  28,  26],\n",
      "        ...,\n",
      "        [ 43,  50,  47],\n",
      "        [ 11,  19,  18],\n",
      "        [ 44,  55,  53]],\n",
      "\n",
      "       [[ 29,  30,  28],\n",
      "        [ 27,  28,  26],\n",
      "        [ 28,  29,  27],\n",
      "        ...,\n",
      "        [ 31,  38,  33],\n",
      "        [ 19,  29,  23],\n",
      "        [ 83,  92,  89]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 94,  83,  69],\n",
      "        [ 94,  83,  69],\n",
      "        [ 92,  81,  67],\n",
      "        ...,\n",
      "        [113, 107, 100],\n",
      "        [107, 101,  94],\n",
      "        [103,  97,  90]],\n",
      "\n",
      "       [[ 89,  78,  64],\n",
      "        [ 88,  77,  63],\n",
      "        [ 85,  74,  60],\n",
      "        ...,\n",
      "        [129, 123, 116],\n",
      "        [129, 123, 116],\n",
      "        [133, 127, 120]],\n",
      "\n",
      "       [[ 83,  72,  58],\n",
      "        [ 81,  70,  56],\n",
      "        [ 77,  66,  52],\n",
      "        ...,\n",
      "        [132, 126, 119],\n",
      "        [129, 123, 116],\n",
      "        [138, 132, 125]]], dtype=uint8)\n",
      "orig_shape: (582, 910)\n",
      "path: 'C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Python Everyday work\\\\Github work\\\\Computer_Vision\\\\YOLO\\\\Test_Image_and_Video\\\\Image.JPG'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': 55.56060001254082, 'inference': 1061.301099951379, 'postprocess': 26.681900024414062}\n"
     ]
    }
   ],
   "source": [
    "detection_output = model.predict(\n",
    "    source=r\"C:\\Users\\Lenovo\\OneDrive\\Desktop\\Python Everyday work\\Github work\\Computer_Vision\\YOLO\\Test_Image_and_Video\\Image.JPG\",\n",
    "    conf=0.25,\n",
    "    save=True\n",
    ")\n",
    "\n",
    "# Display raw tensor output\n",
    "print(detection_output)\n",
    "\n",
    "# Convert first detection result to NumPy\n",
    "print(detection_output[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d3604-d2ed-413b-854d-ac2777dbb4a6",
   "metadata": {},
   "source": [
    "## YOLOv8 Detection Output (Image Detection)\n",
    "\n",
    "This is the detection result for Uploaded Image:\n",
    "\n",
    "![YOLO Image Result](Output/Image.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868795a-2e79-471c-b533-a0b18f3c74d8",
   "metadata": {},
   "source": [
    "# Load Class Labels and Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67182e1-482c-40ad-bd6a-e261b7196b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Lenovo\\OneDrive\\Desktop\\Python Everyday work\\Github work\\Computer_Vision\\YOLO\\Test_Image_and_Video\\Image_1.JPG: 448x640 1 cup, 1 tv, 1 mouse, 1 keyboard, 2 books, 681.0ms\n",
      "Speed: 14.7ms preprocess, 681.0ms inference, 12.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\Lenovo\\OneDrive\\Desktop\\Python Everyday work\\Github work\\Computer_Vision\\YOLO\\runs\\detect\\predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Load COCO class names\n",
    "with open(r\"C:\\Users\\Lenovo\\OneDrive\\Desktop\\Python Everyday work\\Github work\\Computer_Vision\\YOLO\\coco.txt\", \"r\") as f:\n",
    "    class_list = f.read().split(\"\\n\")\n",
    "\n",
    "# Assign random colors for bounding boxes\n",
    "detection_colors = [(random.randint(0,255), random.randint(0,255), random.randint(0,255)) for _ in class_list]\n",
    "\n",
    "# Run YOLO detection on a different image\n",
    "results = model.predict(\n",
    "    source=r\"C:\\Users\\Lenovo\\OneDrive\\Desktop\\Python Everyday work\\Github work\\Computer_Vision\\YOLO\\Test_Image_and_Video\\Image_1.JPG\",\n",
    "    conf=0.25,\n",
    "    save=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06f0ed-6414-4bba-8747-e125799f1444",
   "metadata": {},
   "source": [
    "## YOLOv8 Detection Output (Image Detection)\n",
    "\n",
    "This is the detection result for **Any Image** using COCO class labels:\n",
    "\n",
    "![YOLO Image Result](Output/Image_1.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3506707-942c-497e-ad83-5c8ea980b38c",
   "metadata": {},
   "source": [
    "# Object Detection on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16cc74e4-4623-4dee-b1c8-1cb8931c0078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 5 persons, 2 chairs, 1 laptop, 588.6ms\n",
      "Speed: 17.5ms preprocess, 588.6ms inference, 13.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 chairs, 1 laptop, 511.7ms\n",
      "Speed: 15.8ms preprocess, 511.7ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 1 laptop, 535.4ms\n",
      "Speed: 12.5ms preprocess, 535.4ms inference, 13.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 1 laptop, 611.9ms\n",
      "Speed: 17.4ms preprocess, 611.9ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 4 laptops, 585.3ms\n",
      "Speed: 18.8ms preprocess, 585.3ms inference, 12.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 1 laptop, 461.9ms\n",
      "Speed: 16.3ms preprocess, 461.9ms inference, 11.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 2 laptops, 526.2ms\n",
      "Speed: 18.5ms preprocess, 526.2ms inference, 14.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 2 laptops, 529.5ms\n",
      "Speed: 20.3ms preprocess, 529.5ms inference, 13.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 2 laptops, 602.9ms\n",
      "Speed: 17.6ms preprocess, 602.9ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 2 laptops, 643.2ms\n",
      "Speed: 31.6ms preprocess, 643.2ms inference, 14.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 2 laptops, 391.1ms\n",
      "Speed: 18.1ms preprocess, 391.1ms inference, 11.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 2 laptops, 397.3ms\n",
      "Speed: 17.3ms preprocess, 397.3ms inference, 12.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 3 laptops, 452.6ms\n",
      "Speed: 17.5ms preprocess, 452.6ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 3 laptops, 385.9ms\n",
      "Speed: 17.8ms preprocess, 385.9ms inference, 41.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 3 laptops, 413.7ms\n",
      "Speed: 15.0ms preprocess, 413.7ms inference, 16.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 chairs, 3 laptops, 861.1ms\n",
      "Speed: 22.4ms preprocess, 861.1ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 4 laptops, 687.8ms\n",
      "Speed: 19.8ms preprocess, 687.8ms inference, 19.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 3 laptops, 1607.5ms\n",
      "Speed: 21.6ms preprocess, 1607.5ms inference, 69.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 3 laptops, 725.1ms\n",
      "Speed: 24.1ms preprocess, 725.1ms inference, 14.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 4 laptops, 737.5ms\n",
      "Speed: 19.9ms preprocess, 737.5ms inference, 15.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 662.4ms\n",
      "Speed: 21.4ms preprocess, 662.4ms inference, 19.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 859.1ms\n",
      "Speed: 23.0ms preprocess, 859.1ms inference, 16.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 877.1ms\n",
      "Speed: 17.6ms preprocess, 877.1ms inference, 15.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 1069.3ms\n",
      "Speed: 15.8ms preprocess, 1069.3ms inference, 74.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 652.0ms\n",
      "Speed: 29.3ms preprocess, 652.0ms inference, 14.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 765.5ms\n",
      "Speed: 42.4ms preprocess, 765.5ms inference, 15.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 450.8ms\n",
      "Speed: 17.2ms preprocess, 450.8ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 494.1ms\n",
      "Speed: 19.7ms preprocess, 494.1ms inference, 15.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 471.0ms\n",
      "Speed: 15.5ms preprocess, 471.0ms inference, 17.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 10 laptops, 1067.6ms\n",
      "Speed: 21.2ms preprocess, 1067.6ms inference, 12.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 463.6ms\n",
      "Speed: 15.7ms preprocess, 463.6ms inference, 15.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 7 laptops, 547.5ms\n",
      "Speed: 15.5ms preprocess, 547.5ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 4 laptops, 527.5ms\n",
      "Speed: 16.9ms preprocess, 527.5ms inference, 13.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 6 laptops, 655.8ms\n",
      "Speed: 28.1ms preprocess, 655.8ms inference, 20.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 464.6ms\n",
      "Speed: 19.0ms preprocess, 464.6ms inference, 12.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 2 laptops, 400.9ms\n",
      "Speed: 16.4ms preprocess, 400.9ms inference, 12.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 4 laptops, 404.7ms\n",
      "Speed: 15.9ms preprocess, 404.7ms inference, 12.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 3 laptops, 416.9ms\n",
      "Speed: 15.5ms preprocess, 416.9ms inference, 12.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 1 laptop, 411.1ms\n",
      "Speed: 14.7ms preprocess, 411.1ms inference, 12.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 3 laptops, 518.7ms\n",
      "Speed: 14.6ms preprocess, 518.7ms inference, 17.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 3 laptops, 1179.2ms\n",
      "Speed: 15.2ms preprocess, 1179.2ms inference, 15.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 577.5ms\n",
      "Speed: 21.1ms preprocess, 577.5ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 5 laptops, 777.0ms\n",
      "Speed: 17.6ms preprocess, 777.0ms inference, 13.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 6 laptops, 765.2ms\n",
      "Speed: 29.7ms preprocess, 765.2ms inference, 12.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 4 laptops, 595.3ms\n",
      "Speed: 18.1ms preprocess, 595.3ms inference, 12.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 5 laptops, 766.1ms\n",
      "Speed: 19.3ms preprocess, 766.1ms inference, 13.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 4 laptops, 616.9ms\n",
      "Speed: 26.8ms preprocess, 616.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 4 laptops, 651.6ms\n",
      "Speed: 16.0ms preprocess, 651.6ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 3 laptops, 465.1ms\n",
      "Speed: 15.9ms preprocess, 465.1ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 3 laptops, 684.0ms\n",
      "Speed: 17.7ms preprocess, 684.0ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 5 laptops, 467.4ms\n",
      "Speed: 17.2ms preprocess, 467.4ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 3 laptops, 645.4ms\n",
      "Speed: 17.5ms preprocess, 645.4ms inference, 19.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 3 laptops, 715.2ms\n",
      "Speed: 30.9ms preprocess, 715.2ms inference, 20.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 3 laptops, 459.1ms\n",
      "Speed: 18.4ms preprocess, 459.1ms inference, 22.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 2 laptops, 511.7ms\n",
      "Speed: 19.3ms preprocess, 511.7ms inference, 16.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 2 laptops, 620.6ms\n",
      "Speed: 19.0ms preprocess, 620.6ms inference, 15.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 3 laptops, 450.2ms\n",
      "Speed: 17.2ms preprocess, 450.2ms inference, 17.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 2 laptops, 466.1ms\n",
      "Speed: 15.4ms preprocess, 466.1ms inference, 19.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 2 laptops, 560.2ms\n",
      "Speed: 15.4ms preprocess, 560.2ms inference, 13.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 1 laptop, 423.8ms\n",
      "Speed: 22.3ms preprocess, 423.8ms inference, 13.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 2 laptops, 673.3ms\n",
      "Speed: 15.3ms preprocess, 673.3ms inference, 12.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 1 laptop, 410.1ms\n",
      "Speed: 17.8ms preprocess, 410.1ms inference, 11.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 2 laptops, 489.0ms\n",
      "Speed: 32.6ms preprocess, 489.0ms inference, 13.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 1 laptop, 411.0ms\n",
      "Speed: 15.5ms preprocess, 411.0ms inference, 12.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 1 laptop, 407.0ms\n",
      "Speed: 17.7ms preprocess, 407.0ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 2 laptops, 417.6ms\n",
      "Speed: 18.4ms preprocess, 417.6ms inference, 12.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 2 laptops, 475.4ms\n",
      "Speed: 17.4ms preprocess, 475.4ms inference, 13.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 3 laptops, 404.3ms\n",
      "Speed: 17.1ms preprocess, 404.3ms inference, 13.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 4 laptops, 432.2ms\n",
      "Speed: 15.7ms preprocess, 432.2ms inference, 11.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 chairs, 3 laptops, 401.1ms\n",
      "Speed: 17.2ms preprocess, 401.1ms inference, 13.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 3 laptops, 404.4ms\n",
      "Speed: 15.5ms preprocess, 404.4ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 2 chairs, 2 laptops, 449.3ms\n",
      "Speed: 18.8ms preprocess, 449.3ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 2 laptops, 421.0ms\n",
      "Speed: 17.4ms preprocess, 421.0ms inference, 14.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 3 laptops, 435.4ms\n",
      "Speed: 18.0ms preprocess, 435.4ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 chairs, 3 laptops, 424.9ms\n",
      "Speed: 14.6ms preprocess, 424.9ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 2 laptops, 610.1ms\n",
      "Speed: 21.0ms preprocess, 610.1ms inference, 9.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 3 laptops, 423.7ms\n",
      "Speed: 18.0ms preprocess, 423.7ms inference, 14.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 chairs, 3 laptops, 427.4ms\n",
      "Speed: 14.6ms preprocess, 427.4ms inference, 14.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 chairs, 3 laptops, 409.6ms\n",
      "Speed: 17.1ms preprocess, 409.6ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 chairs, 2 laptops, 405.1ms\n",
      "Speed: 15.5ms preprocess, 405.1ms inference, 12.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 chairs, 2 laptops, 555.5ms\n",
      "Speed: 22.3ms preprocess, 555.5ms inference, 16.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 chairs, 2 laptops, 435.4ms\n",
      "Speed: 17.5ms preprocess, 435.4ms inference, 13.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 chairs, 2 laptops, 1388.2ms\n",
      "Speed: 56.8ms preprocess, 1388.2ms inference, 14.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 chairs, 2 laptops, 1372.2ms\n",
      "Speed: 116.9ms preprocess, 1372.2ms inference, 16.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 chairs, 2 laptops, 454.6ms\n",
      "Speed: 20.7ms preprocess, 454.6ms inference, 14.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import random\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolov8n.pt\", \"v8\")\n",
    "\n",
    "# Load COCO classes\n",
    "with open(r\"C:\\Users\\Lenovo\\OneDrive\\Desktop\\Python Everyday work\\Github work\\Computer_Vision\\YOLO\\coco.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    class_list = f.read().splitlines()\n",
    "\n",
    "# Assign random unique colors for each class\n",
    "detection_colors = [(random.randint(0,255), random.randint(0,255), random.randint(0,255)) for _ in class_list]\n",
    "\n",
    "# Open a video file\n",
    "cap = cv2.VideoCapture(r\"C:\\Users\\Lenovo\\OneDrive\\Desktop\\Python Everyday work\\Github work\\Computer_Vision\\YOLO\\Test_Image_and_Video\\Video.mp4\")\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open video\")\n",
    "    exit()\n",
    "\n",
    "# Create a resizable window with minimize/maximize/close buttons\n",
    "cv2.namedWindow(\"YOLOv8 Object Detection\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Optionally maximize window (depends on OS support)\n",
    "cv2.setWindowProperty(\"YOLOv8 Object Detection\", cv2.WND_PROP_AUTOSIZE, cv2.WINDOW_NORMAL)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of stream\")\n",
    "        break\n",
    "\n",
    "    # Run YOLO prediction\n",
    "    results = model.predict(source=[frame], conf=0.45, save=False)\n",
    "    detections = results[0].numpy()\n",
    "\n",
    "    if len(detections) != 0:\n",
    "        for i in range(len(results[0])):\n",
    "            boxes = results[0].boxes\n",
    "            box = boxes[i]  \n",
    "            clsID = int(box.cls.numpy()[0])\n",
    "            conf = box.conf.numpy()[0]\n",
    "            bb = box.xyxy.numpy()[0]\n",
    "\n",
    "            # Different color for each object class\n",
    "            color = detection_colors[clsID]\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (int(bb[0]), int(bb[1])), (int(bb[2]), int(bb[3])), color, 2)\n",
    "\n",
    "            # Put class name + confidence\n",
    "            cv2.putText(frame, f\"{class_list[clsID]} {round(conf,2)}\", \n",
    "                        (int(bb[0]), int(bb[1]) - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show video in a resizable window (with minimize/maximize/close)\n",
    "    cv2.imshow(\"YOLOv8 Object Detection\", frame)\n",
    "\n",
    "    # Exit on ESC key\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6228ac-8b7b-489a-a8bc-6f5846918da3",
   "metadata": {},
   "source": [
    "## YOLOv8 Detection Output (Video Detection)\n",
    "\n",
    "This is the detection result for the video (screenshot saved as **Video.png**):\n",
    "\n",
    "![YOLO Video Result](Output/Video.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa207e-b7a7-4314-8c07-d3124e52920e",
   "metadata": {},
   "source": [
    "#  Conclusion – Day82 YOLOv8 Project\n",
    "\n",
    "In this notebook, we successfully:\n",
    "\n",
    "-  Installed and set up **Ultralytics YOLOv8**  \n",
    "-  Compared YOLO with Haar Cascades (OpenCV)  \n",
    "-  Used **pretrained models on COCO dataset**  \n",
    "-  Detected objects in **images** and **videos**  \n",
    "-  Displayed results directly inside the notebook  \n",
    "\n",
    "## Key Insights:\n",
    "\n",
    "- YOLOv8 is **faster and more accurate** than traditional methods like Haar Cascades.  \n",
    "- Pretrained models on COCO allow instant use for 80+ classes.  \n",
    "- With simple code, YOLOv8 can handle both **images** and **video streams**.  \n",
    "\n",
    "##  Next Steps:\n",
    "\n",
    "- Try YOLOv8 with **live webcam detection**  \n",
    "- Fine-tune YOLOv8 on a **custom dataset** for your own project  \n",
    "- Explore advanced tasks like **segmentation** and **pose estimation**  \n",
    "\n",
    "Thanks for following along! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
